
# COVID-19 Data Dashboard using PySpark and Power BI

## ğŸ”§ Built With:
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![PySpark](https://img.shields.io/badge/PySpark-FDEE21?style=for-the-badge&logo=apache-spark&logoColor=black)
![Power BI](https://img.shields.io/badge/Power_BI-F2C811?style=for-the-badge&logo=powerbi&logoColor=black)

## ğŸ›¢ï¸ Database Used:
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-316192?style=for-the-badge&logo=postgresql&logoColor=white)

## â˜ï¸ Data Source:
[Our World in Data - COVID-19](https://ourworldindata.org/covid-data)


## ğŸŒ Domain:
![Data Engineering](https://img.shields.io/badge/Data_Engineering-FF6F61?style=for-the-badge&logo=data&logoColor=white)
![Data Analytics](https://img.shields.io/badge/Data%20Analytics-3399FF?style=for-the-badge)

---

## ğŸ“¦ Compressed Data

The dataset used for this ETL pipeline and Power BI dashboard is stored in a compressed format to reduce size and improve portability.

- **File Name:** `compact_covid_data.csv.gz`
- **Format:** GZIP-compressed CSV
- **Original Size:** ~137 MB
- **Compressed Size:** ~[Insert actual compressed size]

âš ï¸ If you download this file manually, please extract it before loading into any tools that do not support `.gz` files directly.

## ğŸ› ï¸ How the ETL Works:

This project performs a basic **ETL (Extract-Transform-Load)** process on worldwide COVID-19 data using PySpark.

### ğŸ”¹ Extract
- Downloads CSV from OWID.
- Saves into a local `/data/` directory.
- Uses PySpark to read the file as a DataFrame.

### ğŸ”¹ Transform
- Filters nulls and invalid rows.
- Selects relevant columns like `location`, `date`, `total_cases`, `total_deaths`, `total_vaccinations`.
- Converts date columns to proper format and casts numeric fields.

### ğŸ”¹ Load
- Data is saved in two formats:
  - Locally as a CSV file (`compact_covid_data.csv`)
  - To **PostgreSQL** database hosted on **Supabase (Vercel Cloud)** via **JDBC**.
- The `.jar` file for PostgreSQL connector is referenced while running the script in Google Colab.

âš ï¸ **Note:** This pipeline is **NOT automated** due to the live nature of the data and inconsistent updates from the OWID dataset.

## ğŸ“Š Power BI Integration:

### ğŸ” Connection:
- Connected to **PostgreSQL** via **ODBC driver**.
- ODBC setup includes:
  - DSN configuration
  - PostgreSQL username/password
  - `SSL Mode: require` to handle remote certificate

### ğŸ§ª Troubleshooting Faced:
- Fixed timeout issues by optimizing queries.
- Ensured data type recognition in Power BI's Power Query Editor.
- Casted blank strings to NULL for proper aggregation.

## ğŸ“¸ Dashboard Preview

![Dashboard](covid_dashboard.png)

This dashboard shows:
- ğŸŒ Total vaccinations and cases by country
- ğŸ“ˆ Time series of cases vs vaccinations
- ğŸ” Filters for continent, location, date
- âœ… Interactive visuals powered via Power BI slicers

## ğŸ“¥ Requirements (`requirements.txt`):

```
pyspark==3.4.1
requests
psycopg2-binary
```

---
## ğŸ“ Folder Structure

```
covid_etl_project/
â”‚
â”œâ”€â”€ covid_etl_package/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ extract.py
â”‚   â”œâ”€â”€ transform.py
â”‚   â”œâ”€â”€ load.py
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ compact_covid_data.csv.gz
â”‚
â”œâ”€â”€ dashboard.pbix
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
```

---

## ğŸ“ Summary:

This project demonstrates a complete **end-to-end data engineering and visualization solution**:
- âœ… Built in **Colab** (Python & PySpark)
- âœ… Stored on **Supabase PostgreSQL**
- âœ… Visualized using **Power BI Desktop**
- âŒ Not automated (due to OWID data reliability)
---


## ğŸš« Note

- Data source may not update regularly.
- This is a one-time ETL and dashboard pipeline.
- Scheduling is **not implemented** due to static data.

